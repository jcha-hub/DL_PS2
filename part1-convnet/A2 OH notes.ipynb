{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-15T23:12:02.937977Z",
     "start_time": "2025-09-15T23:12:02.934369Z"
    }
   },
   "source": [
    "#A2 Pytorch tutorial farrukh 9-15-25-----------\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T23:09:27.435794Z",
     "start_time": "2025-09-15T23:09:27.372060Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# only need to write forward, backward implemented for us autograd computes for us\n",
    "x = torch.tensor([0., 1., 2.])\n",
    "z = torch.rand(size=(3,3))\n",
    "print(x, z)"
   ],
   "id": "aa73a1cbb23692cf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 2.]) tensor([[0.8889, 0.1718, 0.7258],\n",
      "        [0.4293, 0.6115, 0.7300],\n",
      "        [0.7343, 0.2274, 0.9296]])\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T23:14:33.388057Z",
     "start_time": "2025-09-15T23:14:33.382487Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#if want to use accelerator, where data is matters need data and weights in same place\n",
    "# device = torch.accelerator.current_accelerator().type  #returns None\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"device: \", device)\n",
    "\n",
    "z.to(device)\n",
    "print(z.device)"
   ],
   "id": "796e82c697166b3a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device:  cpu\n",
      "cpu\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T23:25:04.260744Z",
     "start_time": "2025-09-15T23:25:04.253211Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x = torch.tensor([2.,3.], requires_grad=True)\n",
    "z = x[0]**2 + x[1]**3 + 1   #yay, do not need to use torch.pow\n",
    "y = 2*z\n",
    "\n",
    "#cannot do x.grad_fn , to save memory, it does not retain grads unless explicitly\n",
    "print(x.grad)\n",
    "print(y.grad_fn)\n",
    "print(y.grad_fn.next_functions)\n",
    "print(x.grad)\n",
    "\n",
    "y.backward()        #pretend is loss function, calculated gradient with input [2,3]\n",
    "print(x.grad)"
   ],
   "id": "8dd21541176217a8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "<MulBackward0 object at 0x0000022F4CE91810>\n",
      "((<AddBackward0 object at 0x0000022F4E2A5120>, 0), (None, 0))\n",
      "None\n",
      "tensor([ 8., 54.])\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T23:26:21.470056Z",
     "start_time": "2025-09-15T23:26:21.451190Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x = torch.tensor([-1.], requires_grad = True)\n",
    "if x.item() > 0:\n",
    "    y = 2 *x\n",
    "else:\n",
    "    y = torch.exp(x)\n",
    "print(x.grad)\n",
    "y.backward()\n",
    "print(x.grad)"
   ],
   "id": "49879bdfd0e1f31c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "tensor([0.3679])\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T23:28:41.134969Z",
     "start_time": "2025-09-15T23:28:41.129280Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#can pull out values from graph\n",
    "z  = x\n",
    "print(z.requires_grad)\n",
    "z = x.detach()\n",
    "print(z.requires_grad)"
   ],
   "id": "7104feaf349d3724",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T23:32:05.880192Z",
     "start_time": "2025-09-15T23:32:05.867274Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#somes we do not want gradients, typically during evaluation, testing, deployment\n",
    "y = x * 2\n",
    "z = y**2\n",
    "print(y.requires_grad)\n",
    "\n",
    "with torch.no_grad():       #does not track gradient here\n",
    "    y = x * 2\n",
    "    print(y.requires_grad)\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(dataset):\n",
    "    pass\n"
   ],
   "id": "7881047d9e7a3796",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T23:45:45.067669Z",
     "start_time": "2025-09-15T23:45:45.061557Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#define network - those auto choose requires_grad\n",
    "import torch.nn as nn\n",
    "\n",
    "class net(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_features=input_dim, out_features=hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        #second layer does not have bias term\n",
    "        self.fc2 = nn.Linear(in_features=hidden_dim, out_features=output_dim, bias=False)\n",
    "        # self.conv2 = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, stride=1, padding=0)\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = net(input_dim=10, hidden_dim=4, output_dim=1)\n",
    "print(model)\n",
    "# print(list(model.parameters()))\n"
   ],
   "id": "b2527ea2bdc420c6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "net(\n",
      "  (fc1): Linear(in_features=10, out_features=4, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (fc2): Linear(in_features=4, out_features=1, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T23:48:32.030369Z",
     "start_time": "2025-09-15T23:48:32.024019Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x = torch.rand(size=(1,10))\n",
    "out = model(x)\n",
    "print(out)\n",
    "\n",
    "#define loss function and optimizer\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1)\n",
    "\n",
    "loss = loss_fn(out, torch.tensor([1.]))\n",
    "print(loss)"
   ],
   "id": "8df1b1bdf8a3a3a9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1404]], grad_fn=<MmBackward0>)\n",
      "tensor(1.3006, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T23:50:18.276619Z",
     "start_time": "2025-09-15T23:50:18.262073Z"
    }
   },
   "cell_type": "code",
   "source": [
    "loss.backward()\n",
    "for p in model.parameters():    #outputs weights, gradients=None\n",
    "    #until we call backward\n",
    "    print(p, p.grad)"
   ],
   "id": "896cced88dd26dd4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0234,  0.0420,  0.1273,  0.2781,  0.1870, -0.2529, -0.2307,  0.0967,\n",
      "          0.1767, -0.2488],\n",
      "        [-0.1831, -0.2042,  0.1716,  0.0372,  0.1465,  0.1246,  0.0431,  0.1743,\n",
      "         -0.1790,  0.2722],\n",
      "        [ 0.1703, -0.1427,  0.2583, -0.2483,  0.1085,  0.2157,  0.0765,  0.2678,\n",
      "          0.0635, -0.2716],\n",
      "        [ 0.2043, -0.0826, -0.0623,  0.2113, -0.2073, -0.3009,  0.0860,  0.1551,\n",
      "          0.0514,  0.2888]], requires_grad=True) tensor([[ 1.0269,  0.5199,  0.9421,  0.3361,  0.4108,  0.0890,  0.3362,  0.7882,\n",
      "          0.8540,  0.2119],\n",
      "        [-0.5657, -0.2864, -0.5190, -0.1852, -0.2263, -0.0490, -0.1852, -0.4342,\n",
      "         -0.4705, -0.1167],\n",
      "        [ 0.1272,  0.0644,  0.1167,  0.0416,  0.0509,  0.0110,  0.0416,  0.0976,\n",
      "          0.1058,  0.0262],\n",
      "        [-0.6894, -0.3490, -0.6325, -0.2257, -0.2758, -0.0597, -0.2257, -0.5291,\n",
      "         -0.5733, -0.1422]])\n",
      "Parameter containing:\n",
      "tensor([ 0.1345,  0.1350, -0.1275, -0.0854], requires_grad=True) tensor([ 1.0856, -0.5981,  0.1344, -0.7288])\n",
      "Parameter containing:\n",
      "tensor([[-0.4760,  0.2622, -0.0589,  0.3195]], requires_grad=True) tensor([[-1.0682, -0.3245, -0.8812, -0.4851]])\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T23:53:24.095153Z",
     "start_time": "2025-09-15T23:53:24.077806Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#everytime you type loss.backward, it keeps acculumulating until you zero out\n",
    "\n",
    "#this updates weights, wow weights have changed\n",
    "optimizer.step()\n",
    "for p in model.parameters():\n",
    "    print(p, p.grad)"
   ],
   "id": "39263871d13e50",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-1.0503, -0.4778, -0.8148, -0.0581, -0.2238, -0.3419, -0.5669, -0.6915,\n",
      "         -0.6773, -0.4607],\n",
      "        [ 0.3826,  0.0822,  0.6906,  0.2224,  0.3728,  0.1736,  0.2282,  0.6086,\n",
      "          0.2915,  0.3889],\n",
      "        [ 0.0431, -0.2071,  0.1417, -0.2899,  0.0576,  0.2047,  0.0349,  0.1702,\n",
      "         -0.0422, -0.2979],\n",
      "        [ 0.8937,  0.2664,  0.5702,  0.4369,  0.0684, -0.2412,  0.3116,  0.6843,\n",
      "          0.6247,  0.4311]], requires_grad=True) tensor([[ 1.0269,  0.5199,  0.9421,  0.3361,  0.4108,  0.0890,  0.3362,  0.7882,\n",
      "          0.8540,  0.2119],\n",
      "        [-0.5657, -0.2864, -0.5190, -0.1852, -0.2263, -0.0490, -0.1852, -0.4342,\n",
      "         -0.4705, -0.1167],\n",
      "        [ 0.1272,  0.0644,  0.1167,  0.0416,  0.0509,  0.0110,  0.0416,  0.0976,\n",
      "          0.1058,  0.0262],\n",
      "        [-0.6894, -0.3490, -0.6325, -0.2257, -0.2758, -0.0597, -0.2257, -0.5291,\n",
      "         -0.5733, -0.1422]])\n",
      "Parameter containing:\n",
      "tensor([-0.9510,  0.7331, -0.2619,  0.6434], requires_grad=True) tensor([ 1.0856, -0.5981,  0.1344, -0.7288])\n",
      "Parameter containing:\n",
      "tensor([[0.5923, 0.5867, 0.8222, 0.8046]], requires_grad=True) tensor([[-1.0682, -0.3245, -0.8812, -0.4851]])\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T23:56:58.161022Z",
     "start_time": "2025-09-15T23:56:58.155189Z"
    }
   },
   "cell_type": "code",
   "source": [
    "optimizer.zero_grad()\n",
    "for p in model.parameters():\n",
    "    print(p, p.grad)"
   ],
   "id": "a806979f25bf6726",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-1.0503, -0.4778, -0.8148, -0.0581, -0.2238, -0.3419, -0.5669, -0.6915,\n",
      "         -0.6773, -0.4607],\n",
      "        [ 0.3826,  0.0822,  0.6906,  0.2224,  0.3728,  0.1736,  0.2282,  0.6086,\n",
      "          0.2915,  0.3889],\n",
      "        [ 0.0431, -0.2071,  0.1417, -0.2899,  0.0576,  0.2047,  0.0349,  0.1702,\n",
      "         -0.0422, -0.2979],\n",
      "        [ 0.8937,  0.2664,  0.5702,  0.4369,  0.0684, -0.2412,  0.3116,  0.6843,\n",
      "          0.6247,  0.4311]], requires_grad=True) None\n",
      "Parameter containing:\n",
      "tensor([-0.9510,  0.7331, -0.2619,  0.6434], requires_grad=True) None\n",
      "Parameter containing:\n",
      "tensor([[0.5923, 0.5867, 0.8222, 0.8046]], requires_grad=True) None\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#define network\n",
    "#define loss function, optimizer\n",
    "#push data thru network\n",
    "#compute loss\n",
    "#call backwards() to compute gradients\n",
    "#optimizer.step() to use gradients to update weights\n",
    "#optimizer.zero_grad() to zero out gradients for next batch\n",
    "\n",
    "\n",
    "class net(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_features=input_dim, out_features=hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        #second layer does not have bias term\n",
    "        self.fc2 = nn.Linear(in_features=hidden_dim, out_features=output_dim, bias=False)\n",
    "        # self.conv2 = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, stride=1, padding=0)\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = net(input_dim=10, hidden_dim=4, output_dim=1)\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1)\n",
    "\n",
    "for _ in range(num_epochs):\n",
    "    for b, y in batches:\n",
    "        b.to(device)\n",
    "        out = model(b)\n",
    "        loss = loss_func(out, y)\n",
    "        loss.back_ward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "#evaluation loop\n",
    "\n",
    "#weights and biases, tensorboard, mlflow might be useful for project"
   ],
   "id": "a7f7381f3a3b79b6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
